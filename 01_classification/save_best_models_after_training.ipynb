{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "sys.path.insert(0, \"../pycode\")\n",
    "from models import *\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"max_rows\", 10)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from seaborn import set_style\n",
    "set_style(\"darkgrid\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.externals import joblib  # to save model to disk\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier      # decision tree\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold  # StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=5, random_state=314159, shuffle=True)    # 5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_dataset_df(df_robust, app2metric_dd):\n",
    "    set1 = set(df_robust['appName'])\n",
    "    set2 = set(app2metric_dd.keys())\n",
    "    if set1 == set2: print \"Great! appName(s) are identical!\"\n",
    "        \n",
    "    #\n",
    "    # generate an empty dataframe\n",
    "    #\n",
    "    app_pd_series = app2metric_dd[list(app2metric_dd.keys())[0]] # query the 1st key value\n",
    "    feat_cols = list(app_pd_series.index)\n",
    "\n",
    "    feat_cols.insert(0, 'appName')  # add appName to the beginning of the list\n",
    "    feat_cols.insert(len(feat_cols), 'Robust')\n",
    "\n",
    "    appNum = len(app2metric_dd.keys())\n",
    "    df_feat = pd.DataFrame(index=np.arange(0, appNum), columns=feat_cols)\n",
    "\n",
    "    #\n",
    "    # fill in the data\n",
    "    #\n",
    "    \n",
    "    feat_cols = list(app_pd_series.index)\n",
    "\n",
    "    for idx, row in df_feat.iterrows():\n",
    "        appName, robust = df_robust.loc[idx]['appName'], df_robust.loc[idx]['Robust']\n",
    "        metric_list = app2metric_dd[appName]\n",
    "\n",
    "        # update \n",
    "        df_feat.loc[idx, 'appName'] = appName\n",
    "        df_feat.loc[idx, 'Robust'] = robust\n",
    "\n",
    "        # update other metrics\n",
    "        for metric in feat_cols:\n",
    "            df_feat.loc[idx, metric] = metric_list[metric]\n",
    "            \n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_model_input(df_dataset):\n",
    "    df_X = df_dataset.drop(['appName', 'Robust'], axis=1)\n",
    "    df_y = df_dataset['Robust']\n",
    "    df_y = df_y.astype('int64')  # convert obj to int\n",
    "    return df_X, df_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def GreedySearch(df_X, df_y, fold_k = 3):\n",
    "#     modelErrorList = []\n",
    "    \n",
    "#     # decision tree\n",
    "#     dt_tree_error, dt_tree_params = GreedySearch_dtree(df_X, df_y, fold_k=fold_k)\n",
    "#     print dt_tree_error, dt_tree_params\n",
    "#     modelErrorList.append((dt_tree_error, 'dtree', dt_tree_params))\n",
    "    \n",
    "#     # knn\n",
    "#     knn_error, knn_params = GreedySearch_KNN(df_X, df_y, fold_k=fold_k)\n",
    "#     print knn_error, knn_params\n",
    "#     modelErrorList.append((knn_error, 'knn', knn_params))\n",
    "\n",
    "#     # svm\n",
    "#     svc_error, svc_params = GreedySearch_SVC(df_X, df_y, fold_k=fold_k)\n",
    "#     print svc_error, svc_params\n",
    "#     modelErrorList.append((svc_error, 'svc', svc_params))\n",
    "\n",
    "#     # random forest\n",
    "#     rf_error, rf_params = GreedySearch_RandomForest(df_X, df_y, fold_k=fold_k)\n",
    "#     print rf_error, rf_params\n",
    "#     modelErrorList.append((rf_error, 'random forest', rf_params))\n",
    "\n",
    "#     nn_error, nn_params = GreedySearch_MLP(df_X, df_y, fold_k=fold_k)\n",
    "#     print nn_error, nn_params\n",
    "#     modelErrorList.append((nn_error, 'neural nets', nn_params))\n",
    "\n",
    "#     ada_error, ada_params = GreedySearch_AdaBoost(df_X, df_y, fold_k=fold_k)\n",
    "#     print ada_error, ada_params\n",
    "#     modelErrorList.append((ada_error, 'AdaBoost', ada_params))\n",
    "\n",
    "#     nb_error = GreedySearch_GaussianNB(df_X, df_y, fold_k=fold_k)\n",
    "#     print nb_error\n",
    "#     modelErrorList.append((nb_error, 'GaussianNB', \"\"))\n",
    "\n",
    "#     qda_error = GreedySearch_QDA(df_X, df_y, fold_k=fold_k)\n",
    "#     print qda_error\n",
    "#     modelErrorList.append((qda_error, 'QDA', \"\"))\n",
    "    \n",
    "#     sort_model_by_error = sorted(modelErrorList, key=lambda x: x[0]) #  error is the 1st value\n",
    "    \n",
    "#     # output\n",
    "#     return sort_model_by_error[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_robust = pd.read_csv('../prepare/run2/app_classify.csv')\n",
    "print type(df_robust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "Robust\n",
      "0    53\n",
      "1    26\n",
      "dtype: int64\n",
      "[0.29411764705882354, 0.125, 0.3125, 0.2, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_featall = np.load('../prepare/app2metric_featAll.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_featall)\n",
    "df_dataset.to_csv(\"dataset_using_featall.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "# AdaBoost 0.21299019607843137 {'n_estimators': 100, 'learning_rate': 0.01}\n",
    "\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = AdaBoostClassifier(n_estimators=100,\n",
    "                               learning_rate=0.01,\n",
    "                               random_state=0)\n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'featall_bestmodel.pkl')\n",
    "    \n",
    "print error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featall_bestmodel = joblib.load('featall_bestmodel.pkl')\n",
    "\n",
    "# for train_index, test_index in kf.split(df_X, df_y):\n",
    "#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "#     y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "#     err = metrics.mean_absolute_error(y_test, featall_bestmodel.predict(X_test))\n",
    "#     print err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "Robust\n",
      "0    53\n",
      "1    26\n",
      "dtype: int64\n",
      "[0.29411764705882354, 0.125, 0.3125, 0.2, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_feat64.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_feat64.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "#AdaBoost 0.21299019607843137 {'n_estimators': 100, 'learning_rate': 0.01}\n",
    "\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = AdaBoostClassifier(n_estimators=100,\n",
    "                               learning_rate=0.01,\n",
    "                               random_state=0)\n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'feat64_bestmodel.pkl')\n",
    "    \n",
    "print error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "Robust\n",
      "0    53\n",
      "1    26\n",
      "dtype: int64\n",
      "[0.23529411764705882, 0.3125, 0.3125, 0.13333333333333333, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_feat42.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_feat42.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "\n",
    "# random forest 0.20039215686274509 {'n_estimators': 100, 'max_features': 'log2', 'criterion': 'gini'}\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = RandomForestClassifier(n_estimators=100,\n",
    "                               max_features='log2',\n",
    "                               criterion='gini')\n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'feat42_bestmodel.pkl')\n",
    "    \n",
    "print error_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "Robust\n",
      "0    53\n",
      "1    26\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leiming/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35294117647058826, 0.3125, 0.3125, 0.3333333333333333, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_feat26.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_feat26.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "\n",
    "\n",
    "# neural nets 0.21372549019607842 {'alpha': 1.0, 'activation': 'identity', 'solver': 'sgd', 'hidden_layer_sizes': (60, 60, 60)}\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = MLPClassifier(alpha=1.0, activation='identity', \n",
    "                          solver='sgd', hidden_layer_sizes=(60, 60, 60))\n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'feat26_bestmodel.pkl')\n",
    "    \n",
    "print error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "Robust\n",
      "0    53\n",
      "1    26\n",
      "dtype: int64\n",
      "[0.23529411764705882, 0.5, 0.375, 0.2, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_feat18.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_feat18.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "# neural nets 0.26215686274509803 {'alpha': 1.0, 'activation': 'identity', 'solver': 'adam', 'hidden_layer_sizes': (60, 60, 60)}\n",
    "\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = MLPClassifier(alpha=1.0, activation='identity', \n",
    "                          solver='adam', hidden_layer_sizes=(60, 60, 60))\n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'feat18_bestmodel.pkl')\n",
    "    \n",
    "print error_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "[0.35294117647058826, 0.375, 0.1875, 0.2, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_feat14.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_feat14.csv\", index=False, encoding='utf-8')\n",
    "#print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "#  knn 0.24975490196078431 {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'brute', 'p': 2}\n",
    "\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='brute', p=2) \n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'feat14_bestmodel.pkl')\n",
    "    \n",
    "print error_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "[0.35294117647058826, 0.3125, 0.3125, 0.26666666666666666, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_feat12.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_feat12.csv\", index=False, encoding='utf-8')\n",
    "#print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "# knn 0.27558823529411763 {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'brute', 'p': 1}\n",
    "\n",
    "\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = KNeighborsClassifier(n_neighbors=10, weights='distance', algorithm='brute', p=1) \n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'feat12_bestmodel.pkl')\n",
    "    \n",
    "print error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "[0.29411764705882354, 0.375, 0.25, 0.13333333333333333, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_feat9.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_feat9.csv\", index=False, encoding='utf-8')\n",
    "#print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "# knn 0.23715686274509803 {'n_neighbors': 4, 'weights': 'uniform', 'algorithm': 'brute', 'p': 1}\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = KNeighborsClassifier(n_neighbors=4, weights='uniform', algorithm='brute', p=1) \n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'feat9_bestmodel.pkl')\n",
    "    \n",
    "print error_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featMystic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! appName(s) are identical!\n",
      "[0.23529411764705882, 0.25, 0.375, 0.2, 0.13333333333333333]\n"
     ]
    }
   ],
   "source": [
    "app2metric_feat = np.load('../prepare/app2metric_featMystic.npy').item()\n",
    "df_dataset = gen_dataset_df(df_robust, app2metric_feat)\n",
    "df_dataset.to_csv(\"dataset_using_featMystic.csv\", index=False, encoding='utf-8')\n",
    "#print df_dataset.groupby(\"Robust\").size()\n",
    "\n",
    "df_X, df_y = gen_model_input(df_dataset)\n",
    "\n",
    "# random forest 0.18779411764705883 {'n_estimators': 100, 'max_features': 'log2', 'criterion': 'gini'}\n",
    "\n",
    "error_list = []\n",
    "minError = 1.0\n",
    "for train_index, test_index in kf.split(df_X, df_y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = df_X.loc[train_index], df_X.loc[test_index]\n",
    "    y_train, y_test = df_y.loc[train_index], df_y.loc[test_index]\n",
    "\n",
    "    clsfy = RandomForestClassifier(n_estimators=100,\n",
    "                               max_features='log2',\n",
    "                               criterion='gini')\n",
    "\n",
    "    clsfy.fit(X_train, y_train)\n",
    "    err = metrics.mean_absolute_error(y_test, clsfy.predict(X_test))\n",
    "    error_list.append(err)\n",
    "    \n",
    "    if err < minError: # update classified\n",
    "        minError = err\n",
    "        joblib.dump(clsfy, 'featMystic_bestmodel.pkl')\n",
    "    \n",
    "print error_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
